{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4d709c-451f-40a8-bee2-3929a8b02630",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31m[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "\u001b[1;31m[notice] To update, run: /opt/homebrew/opt/python@3.12/bin/python3.12 -m pip install --upgrade pip. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session with specific configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Sample PySpark Application\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verify Spark session\n",
    "spark\n",
    "\n",
    "\n",
    "import config as conf\n",
    "import Sparkobject as sparkobj\n",
    "from pyspark.sql.functions import sum, avg, count, col, concat, log1p\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    Retail_df = sparkobj.spark.read.options (header= conf.header_value, inferschema= conf.inferschema_value).csv(conf.DataSetPath);\n",
    "\n",
    "    #Retail_df.show()\n",
    "    \n",
    "    \n",
    "\n",
    "#  Pre processsing of data/data cleaning (handling null values, removing duplicate records, handling bad records)\n",
    "\n",
    "#Q1 Explain how you would handle missing values in the Item_Weight column by using the mean of Item_Weight for each Item_Type.\n",
    "\n",
    "    \n",
    "#op1    windowSpec = Window.partitionBy(\"Item_Type\")\n",
    "\n",
    "#op1    Retail_df = Retail_df.withColumn(\"Item_Weight\",\n",
    "#op1            F.when(col(\"Item_Weight\").isNull(),\n",
    "#op1            F.mean(col(\"Item_Weight\")).over(windowSpec))\n",
    "#op1            .otherwise(col(\"Item_Weight\")))\n",
    "#   Retail_df.select('Item_Identifier', 'Item_Weight', 'Item_Type').show();\n",
    "\n",
    "    \n",
    "\n",
    "    mean_item_wt = Retail_df.groupBy(\"Item_Type\") \\\n",
    "            .agg(F.mean(\"Item_Weight\").alias(\"mean_item_wt\"))\n",
    "\n",
    "    Retail_df1 = Retail_df.join(mean_item_wt, \"Item_Type\", \"left\")\n",
    "    \n",
    "    Retail_df1 = Retail_df1.withColumn(\"Item_Weight\",\n",
    "    F.when(F.col(\"Item_Weight\").isNull(),\n",
    "    F.col(\"mean_item_wt\"))\n",
    "    .otherwise(F.col(\"Item_Weight\")))\n",
    "     \n",
    "\n",
    "    Retail_df1.select('Item_Identifier', 'Item_Weight', 'Item_Type').show()\n",
    "                    \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "#Q2 How do you handle missing values in the Outlet_Size column using the mode of Outlet_Size for each Outlet_Type?\n",
    "\n",
    "    mode_outlet_size = Retail_df1.groupBy(\"Outlet_Size\") \\\n",
    "                .count() \\\n",
    "                .orderBy(F.desc(\"count\")) \\\n",
    "                .first()[0]\n",
    "\n",
    "    Retail_df2 = Retail_df1.withColumn(\"Outlet_Size\",\n",
    "             F.when(F.col(\"Outlet_Size\").isNull(),\n",
    "                F.lit(mode_outlet_size))\n",
    "            .otherwise(F.col(\"Outlet_Size\")))\n",
    "    \n",
    "    Retail_df2.select('Item_Identifier','Outlet_Size', 'Outlet_Type').show();\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Q3 Describe the process to standardize the Item_Fat_Content column, converting all variations of 'Low Fat' to a single standard and 'Regular' to another standard.\n",
    "\n",
    "    Retail_df3 = Retail_df2.withColumn(\"Item_Fat_Content\",\n",
    "        F.when(F.col(\"Item_Fat_Content\").isin (['low fat', 'Low Fat', 'LF']), \"Low Fat\")\n",
    "        .when(F.col(\"Item_Fat_Content\").isin (['reg','Regular']), \"Regular\")\n",
    "        .otherwise(F.col(\"Item_Fat_Content\")))\n",
    "\n",
    "\n",
    "    Retail_df3.select('Item_Identifier','Item_Fat_Content').show();\n",
    "    Retail_df3.write.parquet(f\"./{conf.outputPath}/Standardize_ItemFatContent\", mode=\"overwrite\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#Q4 How do you calculate the age of an outlet using the Outlet_Establishment_Year column and the current year (2024)?\n",
    "\n",
    "    current_year = 2024\n",
    "    \n",
    "    Retail_df3 = Retail_df3.withColumn('Outlet_Age',\n",
    "            F.lit(current_year) - F.col('Outlet_Establishment_Year'))\n",
    "\n",
    "    Retail_df3.select('Outlet_Age','Outlet_Establishment_Year').show();\n",
    "\n",
    "\n",
    "\n",
    "#Q5 Explain the use of StringIndexer for encoding categorical columns like Item_Fat_Content, Outlet_Size, and Outlet_Location_Type.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Q6 How do you apply OneHotEncoder to encode the columns Item_Type, Outlet_Identifier, Outlet_Size_Index, and Outlet_Location_Type_Index?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Q7 Write the code to create an interaction feature between Item_Type and Outlet_Identifier.\n",
    "\n",
    "    Retail_df4 = Retail_df3.withColumn(\"Item_Outlet_Interaction\",\n",
    "    concat(col(\"Item_Type\"), F.lit(\"_\"), F.col(\"Outlet_Identifier\")))\n",
    "\n",
    "    Retail_df4.select('Item_Outlet_Interaction').show();\n",
    "    Retail_df4.write.parquet(f\"./{conf.outputPath}/InteractionFeature_ItemOutlet\", mode=\"overwrite\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Q8 How do you assemble the feature columns Item_Weight, Item_Visibility, Item_MRP, Outlet_Age, and the encoded columns into a single feature vector using VectorAssembler\n",
    "\n",
    "\n",
    "\n",
    "#Q9 Explain the purpose of scaling features and demonstrate how to use StandardScaler to scale the assembled feature vector\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Q10    Describe the process of handling outliers in the Item_Visibility column by using the interquartile range (IQR) method. Provide the code for it.\n",
    "\n",
    "    quantiles = Retail_df4.approxQuantile(\"Item_Visibility\", [0.25, 0.75], 0.0)\n",
    "    Q1, Q3 = quantiles[0], quantiles[1]\n",
    "\n",
    "    IQR = Q3-Q1\n",
    "\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    Retail_df5 = Retail_df3.filter((col(\"Item_Visibility\") >= lower_bound) & (col(\"Item_Visibility\") <= upper_bound))\n",
    "\n",
    "    Retail_df5.show();\n",
    "    Retail_df5.write.parquet(f\"./{conf.outputPath}/HandleOutliers_ItemVisibility\", mode=\"overwrite\")\n",
    "\n",
    "\n",
    "#Q11    How would you apply a log transformation to the Item_MRP column and write the corresponding PySpark code?\n",
    "\n",
    "    Retail_df6 = Retail_df4.withColumn(\"Item_MRP_log\", log1p(col(\"Item_MRP\")))\n",
    "    \n",
    "    Retail_df6.select('Item_MRP', 'Item_MRP_log').show();\n",
    "    Retail_df6.write.parquet(f\"./{conf.outputPath}/LogTransformation_ItemMRP\", mode=\"overwrite\")\n",
    "\n",
    "\n",
    "\n",
    "#Q12    How do you bin the Item_Visibility column into categories like 'Low', 'Medium', and 'High'?\n",
    "    \n",
    "    percentiles = Retail_df.approxQuantile(\"Item_Visibility\", [0.33, 0.66], 0.0)\n",
    "    low_threshold = percentiles[0]\n",
    "    high_threshold = percentiles[1]\n",
    "\n",
    "    Retail_df5 = Retail_df5.withColumn(\"Visibility_Category\", \n",
    "        F.when(col(\"Item_Visibility\") <= low_threshold, \"Low\")\n",
    "        .when((col(\"Item_Visibility\") > low_threshold) & (col(\"Item_Visibility\") <= high_threshold), \"Medium\")\n",
    "        .otherwise(\"High\")\n",
    "    )\n",
    "\n",
    "    Retail_df5.select('Item_Visibility','Visibility_Category').show();\n",
    "    Retail_df5.write.parquet(f\"./{conf.outputPath}/RangeCategorization_on_ItemVisibility\", mode=\"overwrite\")\n",
    "    \n",
    "\n",
    "\n",
    "#Q13    Demonstrate the use of StringIndexer to encode the binned Item_Visibility feature.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Q14    Write the PySpark code to compute the total sales, average sales, and total items sold grouped by Item_Type.\n",
    "\n",
    "    Retail_df6 = Retail_df5.groupBy(\"Item_Type\").agg(sum('Item_MRP').alias('Total_Sales'),\n",
    "                avg('Item_MRP').alias('Average_Sales'),\n",
    "                count('Item_Identifier').alias('Total_items_sold'))\n",
    "\n",
    "    Retail_df6.show();\n",
    "    Retail_df6.write.parquet(f\"./{conf.outputPath}/SalesDetails_based_Item_Type\", mode=\"overwrite\")\n",
    "    \n",
    "\n",
    "#Q15    Write the PySpark code to compute the total sales, average sales, and total items sold grouped by Outlet_Identifier.\n",
    "\n",
    "    Retail_df7 = Retail_df5.groupBy(\"Outlet_Identifier\").agg(sum('Item_MRP').alias('Total_Sales'),\n",
    "                avg('Item_MRP').alias('Average_Sales'),\n",
    "                count('Item_Identifier').alias('Total_items_sold'))\n",
    "\n",
    "    Retail_df7.show();\n",
    "    Retail_df7.write.parquet(f\"./{conf.outputPath}/SalesDetails_based_Outlet_Identifier\", mode=\"overwrite\")\n",
    "\n",
    "\n",
    "\n",
    "#Q16    How do you calculate the average sales per year based on Outlet_Establishment_Year?\n",
    "\n",
    "\n",
    "    Retail_df8 = Retail_df5.groupBy(\"Outlet_Establishment_Year\").agg(avg('Item_MRP').alias('Average_Sales')).orderBy(\"Outlet_Establishment_Year\")\n",
    "\n",
    "    Retail_df8.write.parquet(f\"./{conf.outputPath}/AverageSales_basedOn_Outlet_Establishment_Year\", mode=\"overwrite\")\n",
    "\n",
    "\n",
    "#Q17    Write the PySpark code to compute the total sales, average sales, and total items sold grouped by Item_Fat_Content\n",
    "\n",
    "    \n",
    "    Retail_df9 = Retail_df5.groupBy(\"Item_Fat_Content\").agg(sum('Item_MRP').alias('Total_Sales'),\n",
    "                avg('Item_MRP').alias('Average_Sales'),\n",
    "                count('Item_Identifier').alias('Total_items_sold'))\n",
    "\n",
    "    Retail_df9.show();\n",
    "    Retail_df9.write.parquet(f\"./{conf.outputPath}/Sales_based_Item_Fat_Content\", mode=\"overwrite\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc87847-d56e-40de-ab0a-253347e484ee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c84ea6-4b17-448c-9433-7eafd1f65ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dbb102-4ebb-4dff-a53c-c826021b1774",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4 (main, Jun  6 2024, 18:26:44) [Clang 15.0.0 (clang-1500.3.9.4)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
